\documentclass[10pt]{beamer}
\usetheme{Singapore}
\usecolortheme{default}
\usecolortheme{orchid}
\useoutertheme{infolines}
\useinnertheme[shadow=true]{rounded}

\newcommand\tinyfont{\fontsize{4pt}{7.2}\selectfont}
\newcommand\smallfont{\fontsize{8pt}{7.2}\selectfont}
\newcommand\regfont{\fontsize{10pt}{7.2}\selectfont}

\usepackage{multimedia}
\usepackage{pdfpages}

\title{Using the Yale HPC Clusters}
\titlegraphic{\includegraphics[height=2.0cm]{../logo.png}}
\logo{\includegraphics[height=0.5cm]{../logo.png}}
\author{{Stephen Weston} \and {Robert Bjornson}}
\institute[Yale]{
  Yale Center for Research Computing \\
  Yale University
}
\date{Sep 2016}
\begin{document}


%----------- titlepage ----------------------------------------------%
\begin{frame}[plain]
  \titlepage
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{To get help}
\begin{itemize}
\item Send an email to: \url{hpc@yale.edu}
\item Read documentation at: \url{http://research.computing.yale.edu/hpc-support}
\item Email us directly:
\begin{itemize}
\item Stephen.weston@yale.edu, Office hours at CSSSI on Wednesday morning from 9 to 12 or by appointment
\item Robert.bjornson@yale.edu, By appointment
\end{itemize}
\end{itemize}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{What is the Yale Center for Research Computing?}

\begin{itemize}
\item Specifically created to support your research computing needs
\item \textasciitilde 15 staff, including applications specialists and system engineers
\item Available to consult with and educate users
\item Manage compute clusters and support users
\item Located at 160 St. Ronan st, at the corner of Edwards and St. Ronan
\end{itemize}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{What is a cluster?}

A cluster usually consists of a hundred to a thousand rack mounted
computers, called \textbf{nodes}.  It has one or two login nodes
that are externally accessible, but most of the nodes are
compute nodes and are only accessed from a login node via a
batch queueing system (BQS), also called a \textbf{job scheduler}.

\vskip10pt
The CPU used in clusters may be similar to the CPU in your
desktop computer, but in other respects they are rather different.

\begin{itemize}
\item Linux operating system
\item Command line oriented 
\item Many cores (cpus) per node
\item No monitors, no CD/DVD drives, no audio or video cards
\item Very large distributed file system(s)
\item Connected internally by a fast network
\end{itemize}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Why use a cluster?}
Clusters are very powerful and useful, but it may take some
time to get used to them.
Here are some reasons to go to that effort:

\begin{itemize}
\item Don't want to tie up your own machine for many hours or days
\item Have many long running jobs to run
\item Want to run in parallel to get results quicker
\item Need more disk space
\item Need more memory
\item Want to use software installed on the cluster
\item Want to access data stored on the cluster
\end{itemize}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Limitations of clusters}
Clusters are not the answer to all large scale computing problems.
Some of the limitations of clusters are:

\begin{itemize}
\item Cannot run Windows programs
\item Not for persistent services (DBs or web servers)
\item Not really intended for interactive jobs (especially graphical)
\item Jobs that run for weeks can be a problem (unless checkpointed)
\end{itemize}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Summary of Yale Clusters}
\begin{tabular}{|c|c|c|c|c|}
\hline
& \textbf{Omega} & \textbf{Grace}& \textbf{Louise/Farnam} & \textbf{Ruddle} \\
\hline
\textbf{Role} & FAS & FAS & LS/Med & YCGA  \\
\hline
\textbf{Total nodes} & 1028 & 216 & 115+ & 156+  \\
\hline
\textbf{Cores/node} & 8 & 20 & \multicolumn{2}{c}{mostly 20} \\
\hline
\textbf{Mem/node} & 36 GB/48 GB & 128 GB & \multicolumn{2}{c}{128-1500GB}  \\
\hline
\textbf{Network} & QDR IB & FDR IB& \multicolumn{2}{c}{10 Gb EN}  \\
\hline
\textbf{File system} & Lustre & GPFS & GPFS & NAS + GPFS \\
\hline
\textbf{Batch queueing} & Torque & LSF & Slurm & Torque/Slurm \\
\hline
\end{tabular}

\vskip10pt

Details on each cluster here:
\url{http://research.computing.yale.edu/hpc-clusters}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Setting up a account}

Accounts are free of charge to Yale researchers.
\vskip10pt

Request an account at: \url{http://research.computing.yale.edu/account-request}.

\vskip10pt
After your account has been approved and created, you will receive
an email describing how to access your account.  This may involve setting up ssh keys or 
using a secure login application.  Details vary by cluster.

\vskip10pt
If you need help setting up or using ssh, send an email to: \url{hpc@yale.edu}.
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Ssh to a login node}
To access any of the clusters, you must use \textbf{ssh}.
From a Mac or Linux machine, you simply use the \textbf{ssh} command:

\begin{verbatim}
laptop$ ssh netid@omega.hpc.yale.edu
laptop$ ssh netid@grace.hpc.yale.edu
laptop$ ssh netid@louise.hpc.yale.edu
laptop$ ssh netid@farnam.hpc.yale.edu
laptop$ ssh netid@ruddle.hpc.yale.edu
\end{verbatim}

From a Windows machine, you can choose from programs such as PuTTY or
X-Win32, both of which are available from the Yale Software Library:
\url{http://software.yale.edu}..

\vskip10pt
For more information on using PuTTY (and WinSCP), go to:
\url{http://research.computing.yale.edu/faq\#ssh}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Sshing to Ruddle}

\begin{itemize}
\item Ruddle has an additional level of ssh security, using Multi Factor Authentication (MFA)
\item We use Duo, the same MFA as other secure Yale sites
\end{itemize}

Example:
\begin{verbatim}
bjornson@debian:~$ ssh rdb9@ruddle.hpc.yale.edu
Enter passphrase for key '/home/bjornson/.ssh/id_rsa': 
Duo two-factor login for rdb9

Enter a passcode or select one of the following options:

 1. Duo Push to XXX-XXX-9022
 2. Phone call to XXX-XXX-9022
 3. SMS passcodes to XXX-XXX-9022

Passcode or option (1-3): 1
Success. Logging you in...
\end{verbatim}  
\end{frame}
%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Running jobs on a cluster}

Two ways to run jobs on a cluster:

Interactive:
\begin{itemize}
\item you request an allocation
\item system grants you one or more nodes
\item you are logged onto one of those nodes
\item you run commands
\item you exit and system automatically releases nodes
\end{itemize}

Batch:
\begin{itemize}
\item you write a job script containing commands
\item you request an allocation
\item system grants you one or more nodes
\item your script is automatically run on one of the nodes
\item your script terminates and system releases nodes
item system sends a notification via email
\end{itemize}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Running jobs on a cluster cont.}

Interactive jobs:
\begin{itemize}
\item like a remote session
\item require an active connection
\item for development, debugging, or 
interactive environments like R and Matlab 
\end{itemize}
\vskip10pt

Batch jobs:
\begin{itemize}
\item non-interactive  
\item can run many jobs simultaneously
\item your best choice for production computing
\end{itemize}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Interactive allocations}

\begin{verbatim}

Torque (omega) : qsub -I -q fas_devel
Torque (louise): qsub -I -q general 
LSF    (grace) : bsub -Is -q interactive bash
Slurm  (farnam): srun -p general --pty bash

\end{verbatim}

You'll be logged into a compute node and can run your commands. To exit,
type exit or ctrl-d

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Torque: Example of an interactive job}
\begin{verbatim}
laptop$ ssh sw464@omega.hpc.yale.edu
Last login: Thu Nov  6 10:48:46 2014 from akw105.cs.yale.internal
[ snipped ascii art, etc ]
login-0-0$ qsub -I -q fas_devel
qsub: waiting for job 4347393.rocks.omega.hpc.yale.internal to start
qsub: job 4347393.rocks.omega.hpc.yale.internal ready
compute-34-15$ cd ~/workdir
compute-34-15$ module load Apps/R/3.0.3
compute-34-15$ R --slave -f compute.R
compute-34-15$ exit 
login-0-0$ 
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{LSF: Example of an interactive job}
\begin{verbatim}
laptop$ ssh sw464@grace.hpc.yale.edu
Last login: Thu Nov  6 10:47:52 2014 from akw105.cs.yale.internal
[ snipped ascii art, etc ]
grace1$ bsub -Is -q interactive bash
Job <358314> is submitted to queue.
<<Waiting for dispatch ...>>
<<Starting on c01n01>>
c01n01$ cd ~/workdir
c01n01$ module load Apps/R/3.0.3
c01n01$ R  --slave -f compute.R
c01n01$ exit 
login-0-0$ 
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Slurm: Example of an interactive job}
\begin{verbatim}
farnam-0:~ $ srun -p general --pty bash
c01n01$ module load Apps/R
c01n01$ R --slave -f compute.R
c01n01$ exit
farnam-0:~ $
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example batch script (Torque)}
Here is a very simple batch script that executes an R script.
It can contain PBS directives that embed qsub options
in the script itself, making it easier to submit.  These options
can be overridden by the command line arguments, however.

\begin{block}{}
\begin{verbatim}
#!/bin/bash
#PBS -q fas_normal
#PBS -m abe -M stephen.weston@yale.edu
cd $PBS_O_WORKDIR

module load Apps/R/3.0.3
R --slave -f compute.R
\end{verbatim}
\end{block}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example of a non-interactive/batch job (Torque)}
\smallfont
\begin{verbatim}
login-0-0$ qsub batch.sh
4348736.rocks.omega.hpc.yale.internal
-bash-4.1$ qstat -1 -n 5262300
\end{verbatim}
\tinyfont
\begin{verbatim}
rocks.omega.hpc.yale.internal: 
                                                                                  Req'd    Req'd       Elap
Job ID     Username    Queue    Jobname     SessID  NDS   TSK   Memory   Time    S   Time
-------    ----------- -------- ----------- ------ ----- ------ ------ --------- - ---------
5262300    sw464       fas_norm batch.sh    19620   --     --     --   01:00:00 R  00:00:34   compute-34-15/2

\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example batch script (LSF)}
The same script in LSF:

\begin{block}{}
\begin{verbatim}
#!/bin/bash
#BSUB -q shared

# automatically in submission dir
module load Apps/R/3.0.3
R --slave -f compute.R
\end{verbatim}
\end{block}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example of a batch job (LSF)}
\smallfont
\begin{verbatim}
[rdb9@grace0 ~]$ bsub < batch.sh
Job <113409> is submitted to queue <shared>.
[rdb9@grace0 ~]$ bjobs 113409
JOBID      USER    STAT  QUEUE      FROM_HOST   EXEC_HOST   JOB_NAME   SUBMIT_TIME
113409     rdb9    RUN   shared     grace0      c13n12      *name;date Sep  7 11:18
\end{verbatim}

\regfont
\vskip14pt
Note that ``batch.sh'' was specified via input redirection.
This is required when using BSUB directives in the script,
otherwise the directives will be ignored.
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Slurm: Example of a batch script}
The same script in Slurm:

\begin{block}{}
\begin{verbatim}
#!/bin/bash
#SBATCH -p debug 
#SBATCH -n 20 -N 1
#SBATCH -t 00:30:00  
#SBATCH -J testjob

module load Apps/R
R --slave -f compute.R
\end{verbatim}
\end{block}{}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Slurm: Example of a batch job}
\smallfont
\begin{verbatim}
$ sbatch test.sh
Submitted batch job 42
$ squeue -j 42
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
   42   general   my_job     rdb9  R       0:03      1 c13n10
\end{verbatim}
\vskip14pt
\regfont
The script runs in the current directory.  
Output goes to slurm-\textit{jobid}.out by default, or use -o
\end{frame}


%----------- advanced slides ----------------------------------------%

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Copy scripts and data to the cluster}
\begin{itemize}
\item On Linux and Mac, use scp and rsync
to copy scripts and data files from between local computer and cluster.

\begin{verbatim}
laptop$ scp -r ~/workdir netid@omega.hpc.yale.edu:
laptop$ rsync -av ~/workdir netid@grace.hpc.yale.edu:
\end{verbatim}

\item For Mac, Cyberduck is a good graphical tool.  \url{https://cyberduck.io}

\item For Windows, WinSCP is available from the Yale Software
Library: \url{http://software.yale.edu}.


\item Other windows options include:
\begin{itemize}
\item Bitvise ssh \url{https://www.bitvise.com}
\item Cygwin (Linux emulator in Windows: \url{https://www.cygwin.com}
\end{itemize}
\end{itemize}



\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Summary of Torque commands (Omega/Louise/Ruddle)}
\begin{tabular}{|l|l|}
\hline
\textbf{Description} & \textbf{Command} \\
\hline
Submit a job & qsub [\textit{opts}] \textit{SCRIPT} \\
\hline
Status of a job & qstat \textit{JOBID} \\
\hline
Status of a user's jobs & qstat -u \textit{NETID} \\
\hline
Detailed status of a user's jobs & qstat -1 -n -u \textit{NETID} \\
\hline
Cancel a job & qdel \textit{JOBID} \\
\hline
Get queue info & qstat -Q \\
\hline
Get node info & pbsnodes \textit{NODE} \\
\hline
\end{tabular}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Summary of qsub options}
\begin{tabular}{|l|l|}
\hline
\textbf{Description} & \textbf{Option} \\
\hline
Queue & -q \textit{QUEUE} \\
\hline
Process count & -l nodes=\textit{N}:ppn=\textit{M} \\
\hline
Wall clock limit & -l walltime=\textit{DD}:\textit{HH}:\textit{MM}:\textit{SS} \\
\hline
Memory limit & -l mem=\textit{J}gb \\
\hline
Interactive job & -\texttt{I} \\
\hline
Interactive/X11 job & -\texttt{I} -X \\
\hline
Job name & -N \textit{NAME} \\
\hline
\end{tabular}

\vskip10pt
Examples:
\begin{verbatim}
login-0-0$ qsub -I -q fas_normal -l mem=32gb,walltime=24:00:00
login-0-0$ qsub -q fas_long -l mem=32gb,walltime=3:00:00:00 job.sh
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Summary of LSF commands (Grace)}
\begin{tabular}{|l|l|}
\hline
\textbf{Description} & \textbf{Command} \\
\hline
Submit a job & bsub [\textit{opts}] \textless \textit{SCRIPT} \\
\hline
Status of a job & bjobs \textit{JOBID} \\
\hline
Status of a user's jobs & bjobs -u \textit{NETID} \\
\hline
Cancel a job & bkill \textit{JOBID} \\
\hline
Get queue info & bqueues \\
\hline
Get node info & bhosts \\
\hline
\end{tabular}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Summary of LSF bsub options (Grace)}
\begin{tabular}{|l|l|}
\hline
\textbf{Description} & \textbf{Option} \\
\hline
Queue & -q \textit{QUEUE} \\
\hline
Process count & -n \textit{P} -R "span[hosts=1]" \\
\hline
Process count & -n \textit{P} \\
\hline
Wall clock limit & -W \textit{HH}:\textit{MM} \\
\hline
Memory limit & -M \textit{M} \\
\hline
Interactive job & -\texttt{Is} bash \\
\hline
Interactive/X11 job & -\texttt{Is} -XF bash \\
\hline
Job name & -J \textit{NAME} \\
\hline
\end{tabular}

\vskip10pt
Examples:
\begin{verbatim}
grace1$ bsub -Is -q shared -M 32768 -W 24:00     # 32gb for 1 day
grace1$ bsub -q long -M 32768 -W 72:00 < job.sh  # 32gb for 3 days
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Summary of Slurm commands (Farnam)}
\begin{tabular}{|l|l|}
\hline
\textbf{Description} & \textbf{Command} \\
\hline
Submit a batch job & sbatch [\textit{opts}] \textit{SCRIPT} \\
\hline
Submit an interactive job & srun --pty [\textit{opts}] \textit{COMMAND} \\
\hline
Status of a job & squeue -j \textit{JOBID} \\
\hline
Status of a user's jobs & squeue -u \textit{NETID} \\
\hline
Cancel a job & scancel \textit{JOBID} \\
\hline
Get queue info & squeue -p \textit{PART} \\
\hline
Get node info & sinfo -N \\
\hline
\end{tabular}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Summary of Slurm sbatch/srun options (Farnam)}
\begin{tabular}{|l|l|}
\hline
\textbf{Description} & \textbf{Option} \\
\hline
Partiion & -p \textit{QUEUE} \\
\hline
Process count & -n \textit{Cpus} \\
\hline
Node count & -N \textit{Nodes} \\
\hline
Wall clock limit & -t \textit{HH}:\textit{MM} \\
\hline
Memory limit & --mem-per-cpu \textit{M} (MB)\\
\hline
Interactive job & srun --pty bash \\
\hline
Job name & -J \textit{NAME} \\
\hline
\end{tabular}

\vskip10pt
Examples:
\smallfont
\begin{verbatim}
farnam1$ srun --pty -p general --mem-per-cpu 32768 -t 24:00 bash # 32gb for 1 day
farnam1$ sbatch -p general --mem-per-cpu 32768 -t 72:00 job.sh   # 32gb for 3 days
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Controlling memory usage}
\begin{itemize}
\item It's crucial to understand memory required by your program.
\item Nodes (and thus memory) are often shared
\item Jobs have default memory limits that you can override
\item Each BQS specifies this differently
\end{itemize}

To specify 16 cores with 20 GB each:  
\begin{verbatim}
Torque: qsub -lnodes=2:ppn=8 -lmem=180gb t.sh
LSF: bsub -n 16 -M 20000 t.sh
Slurm: sbatch -n 16 --mem-per-cpu=20000 t.sh

\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Controlling walltime}
\begin{itemize}
\item Each job has a default maximum walltime
\item The job is killed if that is exceeded
\item You can specify longer walltime to prevent being killed
\item You can specify shorter walltime to get resources faster
\end{itemize}

To specify walltime limit of 2 days:
\begin{verbatim}
Torque: qsub -lwalltime=2:00:00:00 t.sh
LSF: bsub -W 48:00 t.sh
Slurm: sbatch -t 2:00:00 t.sh

\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}<presentation:0>
\frametitle{Grace: Notes on memory limits}
The \textbf{-M} option specifies the maximum memory per core in
megabytes.
It is a runtime limit, and is not used for scheduling purposes.
The default value is 5120, or 5 gigabytes per core.

\vskip10pt
The \textbf{-R "rusage[mem=M]"} option specifies the memory to ``reserve''
for the job on each node. It is used for scheduling purposes and has no
effect at runtime.
The default value is 5120, or 5 gigabytes per node.
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}<presentation:0>[fragile]
\frametitle{Omega: Requesting walltime with qsub}
The value of \textbf{walltime} is particularly important because if you
ask for too little time, your job will be killed, but if you ask for too
much, it may not be scheduled to run for a long time.

\begin{itemize}
\item Specified as DD:HH:MM:SS (days, hours, minutes, seconds)
\item Default is one hour
\item Try to determine expected runtime during testing
\end{itemize}

The following requests four hours of time:

\begin{verbatim}
login-0-0$ qsub -q fas_normal -l walltime=4:00:00 batch.sh
\end{verbatim}

If your job checkpoints itself periodically, then this decision is
less critical.
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}<presentation:0>[fragile]
\frametitle{Grace: Requesting walltime with bsub}
You should use the bsub \textbf{-W} option to specify the maximum time
needed by your job. If you exceed this time, your job will be killed.

\begin{itemize}
\item Specified as HH:MM (hours, minutes)
\item Default is one hour
\item Try to determine expected runtime during testing
\end{itemize}

The following requests four hours of time:

\begin{verbatim}
grace1$ bsub -q shared -W 4:00 < batch.sh
\end{verbatim}

If your job checkpoints itself periodically, then this decision is
less critical.
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Submission Queues}

Regardless of which cluster you are on, you will submit jobs to a particular queue, depending on:
\begin{itemize}
\item{Job's requirements}
\item{Access rights for particular queues}
\item{Queue rules}
\end{itemize}

Each cluster has its own queues, with specific rules, such as maximum walltime, cores, nodes, jobs, etc.  
We detail them in the following slides.
\vspace{0.1in}


\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Omega Queues}
You must always specify a queue via the qsub \textbf{-q} argument on
Omega.  The primary factor in choosing a queue is the value of
\textbf{walltime} because the different queues have different
restrictions on how long jobs can run.

\vskip10pt
\begin{description}[fas\_very\_long]
\item[fas\_very\_long]       4 weeks
\item[fas\_long]             3 days
\item[fas\_high]             1 day
\item[fas\_normal]           1 day
\item[fas\_devel]            4 hours
\end{description}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Grace Queues}
The default queue on Grace is \textbf{shared}, but you can specify
a different queue via the bsub \textbf{-q} argument.
The primary factor in choosing a queue is the value of the \textbf{-W}
option because the different queues have different restrictions on how
long jobs can run.

\vskip10pt
\begin{description}[interactive]
\item[long]                  4 weeks
\item[week]                  1 week
\item[shared]                1 day
\item[interactive]           4 hours
\end{description}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Louise Queues}

\begin{itemize}
\item Louise has 5 different nodes types, ranging from 4-64 cores, and 16-512 GB of RAM.  General queue jobs may be run on any
of these node types.  Use -W PARTITION:\textit{type} to select a specific type. 
\end{itemize}

\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Queue} & \textbf{Walltime def/max (days)}& \textbf{MAX jobs/cpus} & \textbf{Notes} \\
\hline
\textbf{general} & 30/30 &  10/64 & default \\
\hline
\textbf{scavenge} & 30/30 &  4/320 & preemptable \\
\hline
\textbf{\textit{PI}} & none & none & members only \\
\hline
\end{tabular}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}

\frametitle{Farnam Queues}

\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Queue} & \textbf{Walltime def/max}& \textbf{MAX cpus} & \textbf{Notes} \\
\hline
\textbf{general} & 7/30 &  100 & default \\
\hline
\textbf{interactive} & 1/1 & 4 &  \\
\hline
\textbf{bigmem} & 1/7 & 64 & 1.5TB RAM \\
\hline
\textbf{gpu} & 1/7 & TBD & 2 GPUS/node \\
\hline
\textbf{scavenge} & 1/30 & 200 & preemptable \\
\hline
\textbf{textit{PI}} & 1/7 & N/A & members only \\
\hline
\end{tabular}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}
\frametitle{Ruddle Queues}

\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Queue} & \textbf{Walltime def/max}& \textbf{MAX cpus} & \textbf{Notes} \\
\hline
\textbf{default} & 7/30 &  300 & default \\
\hline
\textbf{interactive} & 1/1 & 20 &  \\
\hline
\textbf{bigmem} & 7/30 & 64 & 1.5TB RAM \\
\hline
\end{tabular}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Module files}
Much of the HPC software on Omega, Grace, Ruddle, and Farnam 
is installed in non-standard
locations. This makes it easier to to maintain different versions of the same
software allowing users to specify the version of an application that
they wish to use.  This is done with the module command.

\vskip10pt
For example, before you can execute Matlab, you need to initialize your
environment by loading a Matlab ``module file'':

\begin{verbatim}
$ module load Apps/Matlab
\end{verbatim}

This will modify variables in your environment such as PATH and
LD\_LIBRARY\_PATH so that you can execute the \textbf{matlab} command.
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Omega: Finding module files}
There are many module files for the various applications, tools, and
libraries installed on the FAS clusters.  To find the module file that
will allow you to run Matlab, use the `modulefind` command:

\begin{verbatim}
login-0-0$ modulefind matlab
\end{verbatim}

This will produce output like:

\begin{verbatim}
/home/apps/fas/Modules:
Apps/Matlab/R2010b
Apps/Matlab/R2012b
Apps/Matlab/R2014a
\end{verbatim}

You can get a listing of available module files with:
\begin{verbatim}
login-0-0$ module avail
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Grace, Ruddle, Farnam: Finding module files}
There are many module files for the various applications, tools, and
libraries installed on thw clusters.  To find the module file that
will allow you to run Bowtie, use the `module avail` command:

\begin{verbatim}
[rdb9@grace0 ~]$ module avail matlab

---------------------------------------------------------------- /gpfs/apps/hpc/Modules ----------------------------------------------------------------
   Apps/Matlab/R2013b    Apps/Matlab/R2014a    Apps/Matlab/R2015a (D)    Apps/Matlab/R2015b    MatlabPkg/SPM/8    MatlabPkg/SPM/12 (D)

  Where:
   (D):  Default Module

\end{verbatim}

You can get a listing of all available module files with:
\begin{verbatim}
grace1$ module avail
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Example ``module load'' commands}
Here are some ``module load'' commands for scripting languages:

\begin{verbatim}
module load Langs/Python
module load Langs/Perl
module load Apps/R
module load Apps/Matlab
module load Apps/Mathematica
\end{verbatim}

You can specify a specific version:
\begin{verbatim}
module load Langs/Python/2.7.6
\end{verbatim}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Run your program/script}
When you're finally ready to run your script, you may have some
trouble determining the correct command line, especially if you want to
pass arguments to the script.  Here are some examples:

\begin{verbatim}
compute-20-1$ python compute.py input.dat
compute-20-1$ R --slave -f compute.R --args input.dat
compute-20-1$ matlab -nodisplay -nosplash -nojvm < compute.m
compute-20-1$ math -script compute.m
compute-20-1$ MathematicaScript -script compute.m input.dat
\end{verbatim}

You often can get help from the command itself using:

\begin{verbatim}
compute-20-1$ matlab -help
compute-20-1$ python -h
compute-20-1$ R --help
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Running graphical programs on compute nodes}
Two different ways:
\begin{itemize}
\item X11 forwarding
\begin{itemize}
\item easy setup
\item ssh -Y to cluster, then qsub -Y/bsub -XF/srun --x11
\item works fine for most applications
\item bogs down for very rich graphics
\end{itemize}
\item Remote desktop (VNC)
\begin{itemize}
\item more setup
\item allocate node, start VNC server there, connect via ssh tunnels
\item works very well for rich graphics
\end{itemize}
\end{itemize}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}<presentation:0>[fragile]
\frametitle{Running graphical programs (Torque)}
Enable X11 forwarding from ssh and qsub:
\begin{verbatim}
laptop$ ssh -Y netid@omega.hpc.yale.edu
login-0-0$ qsub -X -I -q fas_devel
\end{verbatim}

Faster alternative:
\begin{verbatim}
laptop$ ssh netid@omega.hpc.yale.edu
login-0-0$ qsub -I -q fas_devel
\end{verbatim}

Once job is running, execute from a different terminal window:
\begin{verbatim}
laptop$ ssh -Y netid@omega.hpc.yale.edu
laptop$ ssh -Y compute-XX-YY
\end{verbatim}

where ``compute-XX-YY'' is the node allocated to your job.
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}<presentation:0>[fragile]
\frametitle{Running graphical programs (LSF)}
Enable X11 forwarding from ssh and bsub:
\begin{verbatim}
laptop$ ssh -Y netid@grace.hpc.yale.edu
grace1$ bsub -XF -Is -q interactive bash
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Cluster Filesystems}
Each cluster has a number of different filesystems for you to use, with different
rules and performance characteristics.  

\vskip10pt

It is very important to understand the differences.  Generally, each cluster will have:

\begin{description}
\item[Home]: Backed up, small quota, for scripts, programs, documents, etc.
\item[Scratch]: Not Backed up.  Automatically purged.  For temporary files.
\item[Project]: Not Backed up.  For longer term storage.
\item[Local HD]: /tmp  For local scratch files.
\item[RAMDISK]: /dev/shm For local scratch files.
\item[Storage@Yale]: University-wide storage (active and archive).
\end{description}

Consider using local HD or ramdisk for intermediate files.  Also consider avoiding files by using pipes.

For more info: \url{http://research.computing.yale.edu/hpc/faq/io-tutorial}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Hints for working with YCGA Genomic data}
\begin{itemize}
\item Data layout, and avoiding copies
\item Improving IO
\item Genome reference files
\item Standard processing pipelines
\end{itemize}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Access data directly rather than copying}

You'll typically get an email from YCGA with links:

\begin{verbatim}

http://sysg1.cs.yale.edu:2011/showrun?run=ba_sequencers5/sequencerX 
  /runs/150928_D00536_0191_BC79VRANXX 
Regarding samples:

Sample_7777  â€¦
\end{verbatim}

The sample url looks like:

\begin{verbatim}
http://sysg1.cs.yale.edu:2011/gen?fullPath=ba_sequencers5/sequencerX
  /runs/150928_D00536_0191_BC79VRANXX/Data/Intensities/BaseCalls
  /Unaligned/Project_PI/Sample_7777 ...

\end{verbatim}
\end{frame}


%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Access data directly rather than copying}

Instead of downloading them, you can access them directly on the cluster, by (trivially) 
converting the url to a path:

\begin{verbatim}
cd /ycga-ba/ba_sequencers5/sequencerX
  /runs/150928_D00536_0191_BC79VRANXX/Data/Intensities/BaseCalls
  /Unaligned/Project_PI/Sample_7777
\end{verbatim}

Path conversion rules
\begin{verbatim}
sequencers... -> /panfs/sequencers...
ba_sequencers... -> /ycga-ba/ba_sequencers...
\end{verbatim}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Access data directly rather than copying (cont)}

Your data is in multiple compressed fastq files, typically in a directory
like Unaligned/Project\_PIname/Sample\_samplename.  

\begin{verbatim}
7777_GCCAAT-GCCAAT_L008_R1_001.fastq.gz
7777_GCCAAT-GCCAAT_L008_R1_002.fastq.gz
7777_GCCAAT-GCCAAT_L008_R1_003.fastq.gz
7777_GCCAAT-GCCAAT_L008_R1_004.fastq.gz
...
7777_GCCAAT-GCCAAT_L008_R2_001.fastq.gz
7777_GCCAAT-GCCAAT_L008_R2_002.fastq.gz
7777_GCCAAT-GCCAAT_L008_R2_003.fastq.gz
7777_GCCAAT-GCCAAT_L008_R2_004.fastq.gz
...
\end{verbatim}
\end{frame}


%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Use Links to avoid copying fastq files}
Many users incur a 2-3x space increase:

\begin{verbatim}
$ zcat Sample*_R2_0??.fastq.gz >  /scratchspace/Sample.fastq
\end{verbatim}

Use soft links instead
\begin{verbatim}
$ mkdir Project/Sample_7777
$ cd Project/Sample_7777
$ ln -s /path/to/sample/*.fastq.gz .
\end{verbatim}

Many programs can use compressed split files directly.  If they can't, use this trick
\begin{verbatim}
$ bwa ... <(zcat *R1_*.fastq.gz) <(zcat *R2_*.fastq.gz) 
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Use Pipes to improve performance}

\vskip10pt
Faster: avoids file IO and increases parallelism

\vskip10pt
Using Files
\begin{verbatim}
$ gunzip test_export.txt.gz 
$ perl filter.pl test_export.txt \
  test_filtered.txt
$ perl illumina_export2sam.pl \
  --read1=test_filtered.txt > test_filtered.sam
$ samtools view -bS -t hg19.fa.fai \
  test_filtered.sam -o test_filtered.bam
$ samtools sort test_filtered.bam test_sorted
\end{verbatim}

Using Pipes
\begin{verbatim}
$ gunzip -c test.export.txt.gz \
| perl filter.pl - - \
| perl illumina\_export2sam.pl --read1=- \
| samtools view -bS -t hg19.fa.fai - \
| samtools sort - test.sorted
\end{verbatim}
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Genome references installed on Ruddle}

Please do not install your own copies of popular files (e.g. genome refs).  
\vskip10pt
We have a number of references installed here:
\begin{verbatim}
/home/bioinfo/genomes
\end{verbatim}

If you don't find what you need, please ask us, and we will install them.

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Bioinformatics Assistance}

Contact Jim Knight (j.knight@yale.edu)

http://campuspress.yale.edu/knightlab

The Knight lab has a number of high performance, parallel pipelines for
doing standard analyses:

\begin{itemize}
\item read QC
\item GATK variant detection
\end{itemize}
\end{frame}


%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Wait, where is the Parallelism?}

Qsub or Bsub can allocate multiple nodes, but the script runs on one core on one node sequentially.  How do we use multiple cpus?

\begin{itemize}
\item Submit many batch jobs simultaneously (not good)
\item Use job arrays (better)
\item Submit a parallel version of your program (great if you have one)
\item Use SimpleQueue (excellent)
\end{itemize}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{SimpleQueue}

\begin{itemize}
\item Useful when you have many similar, independent jobs to run
\item Automatically schedules jobs onto a single PBS allocation
\end{itemize}

Advantages

\begin{itemize}
\item Handles startup, shutdown, errors
\item Only one batch job to keep track of
\item Keeps track of status of individual jobs
\item Automatically schedules jobs onto a single PBS allocation
\end{itemize}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Job Arrays}

\begin{itemize}
\item Useful when you have many nearly identical, independent jobs to run
\item Starts many copies of your script, distinquished by a task id.
\end{itemize}

Submit jobs like this:
\begin{verbatim}
Torque: qsub -t 1-100 ...
Slurm: sbatch --array=1-100 ..
LSF: bsub -J "job [1-100]" ...
\end{verbatim}

Script references an environment variable:
\begin{verbatim}
#PBS -t 1-10

cd $PBS_O_WORKDIR

./mycommand -i input.${PBS_ARRAYID} \
    -o output.${PBS_ARRAYID}
\end{verbatim}
Other BQS are similar
\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Using SimpleQueue}

\begin{enumerate}
\item Create file containing list of commands to run (jobs.list)
\begin{verbatim}
cd ~/mydir/myrun; prog arg1 arg2 -o job1.out  
cd ~/mydir/myrun; prog arg1 arg2 -o job2.out  
...
\end{verbatim}
\item Create launch script
\begin{verbatim}
# Louise:
/path/to/sqPBS.py queue Nodes Account Title jobs.list > run.sh
# Omega, Grace, Ruddle, Farnam:
module load Tools/SimpleQueue
sqCreateScript -q queuename -n 4 jobs.list > run.sh
\end{verbatim}

\item Submit launch script
\begin{verbatim}
qsub run.sh    # Louise, Omega, Ruddle
bsub < run.sh  # Grace
sbatch run.sh  # Farnam
\end{verbatim}
\end{enumerate}

For more info, see \url{http://research.computing.yale.edu/hpc/faq/simplequeue}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Best Practices}

\begin{itemize}
\item Start Slowly
\begin{itemize}
\item Run your program on a small dataset interactively
\item In another ssh, watch program with top.  Track memory usage.
\item Check outputs
\item Only then, scale up dataset, convert to batch run
\end{itemize}
\item Input/Output
\begin{itemize}
\item Think about input and output files, number and size
\item Should you use local or ram filesystem? 
\end{itemize}
\item Memory

\begin{itemize}
\item Use top or /usr/bin/time -a to monitor usage
\item Consider using memory option when submitting or allocate entire node.
\end{itemize}

\item Be considerate!  Clusters are shared resources.

\begin{itemize}
\item Don't run programs on the login nodes.  Make sure to allocate a compute node.
\item Don't submit a large number of jobs at once.  Use simplequeue.
\item Don't do heavy IO to /home.
\item Don't fill filesystems.
\end{itemize}
\end{itemize}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Plug for scripting languages}

\begin{itemize}
\item Learning basics of a scripting language is a great investment.
\item Very useful for automating lots of day to day activities
\begin{itemize}
\item Parsing data files
\item Converting file formats
\item Verifying collections of files
\item Creating processing pipelines
\item Summarizing, etc. etc.
\end{itemize}
\item Python (strongly recommended)
\item Bash 
\item Perl (if your lab uses it)
\item R (if you do a lot of statistics or graphing)
\end{itemize}

\end{frame}

%----------- slide --------------------------------------------------%
\begin{frame}[fragile]
\frametitle{Resources}

\begin{itemize}
\item Please feel free to contact us.
\item Online documentation: \url{http://research.computing.yale.edu/hpc}
\item Very useful table of equivalent BQS commands: https://slurm.schedmd.com/rosetta.pdf
\item Recommended Books
\begin{itemize}
\item Learning Python: Mark Lutz
\item Python Cookbook: Alex Martelli
\item Bioinformatics Programming using Python: Mitchell Model
\item Learning Perl: Randal Schwarz
\end{itemize}
\end{itemize}

\end{frame}


\end{document}
